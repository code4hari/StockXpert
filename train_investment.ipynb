{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import TimeSeriesTransformerModel, TimeSeriesTransformerConfig\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import optuna\n",
    "import talib\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DataCollector:\n",
    "    def __init__(self):\n",
    "        self.base_data = {}\n",
    "        self.sentiment_data = {}\n",
    "        self.economic_indicators = {}\n",
    "        \n",
    "    def fetch_historical_data(self, symbols, start_date, end_date):\n",
    "        \"\"\"Fetch historical stock data from multiple sources\"\"\"\n",
    "        print(\"üìä Fetching historical market data...\")\n",
    "        \n",
    "        all_data = {}\n",
    "        for symbol in tqdm(symbols):\n",
    "            # Primary market data from Yahoo Finance\n",
    "            stock = yf.Ticker(symbol)\n",
    "            df = stock.history(start=start_date, end=end_date, interval='1d')\n",
    "            \n",
    "            # Add trading volume analysis\n",
    "            df['Volume_MA'] = df['Volume'].rolling(window=20).mean()\n",
    "            df['Volume_Ratio'] = df['Volume'] / df['Volume_MA']\n",
    "            \n",
    "            # Calculate additional price metrics\n",
    "            df['Daily_Return'] = df['Close'].pct_change()\n",
    "            df['Volatility'] = df['Daily_Return'].rolling(window=20).std()\n",
    "            \n",
    "            all_data[symbol] = df\n",
    "            \n",
    "        self.base_data = all_data\n",
    "        return all_data\n",
    "    \n",
    "    def add_economic_indicators(self):\n",
    "        \"\"\"Add macroeconomic indicators\"\"\"\n",
    "        print(\"üåç Adding economic indicators...\")\n",
    "        \n",
    "        # Simulated economic data (in real implementation, would fetch from FRED API)\n",
    "        dates = pd.date_range(start='2010-01-01', end='2024-01-01', freq='D')\n",
    "        \n",
    "        indicators = {\n",
    "            'GDP_Growth': np.random.normal(2.5, 0.5, len(dates)),\n",
    "            'Inflation_Rate': np.random.normal(2.0, 0.3, len(dates)),\n",
    "            'Unemployment_Rate': np.random.normal(5.0, 0.4, len(dates)),\n",
    "            'Interest_Rate': np.random.normal(3.0, 0.2, len(dates))\n",
    "        }\n",
    "        \n",
    "        self.economic_indicators = pd.DataFrame(indicators, index=dates)\n",
    "        return self.economic_indicators\n",
    "    \n",
    "    def add_sentiment_analysis(self, symbols):\n",
    "        \"\"\"Add sentiment analysis from news and social media\"\"\"\n",
    "        print(\"üó£Ô∏è Performing sentiment analysis...\")\n",
    "        \n",
    "        sentiment_data = {}\n",
    "        for symbol in tqdm(symbols):\n",
    "            # Simulated sentiment scores (in real implementation, would fetch from news APIs)\n",
    "            dates = pd.date_range(start='2010-01-01', end='2024-01-01', freq='D')\n",
    "            \n",
    "            sentiment_scores = {\n",
    "                'News_Sentiment': np.random.normal(0.2, 0.3, len(dates)),\n",
    "                'Social_Media_Score': np.random.normal(0.3, 0.4, len(dates)),\n",
    "                'Analyst_Rating': np.random.normal(3.5, 0.5, len(dates))\n",
    "            }\n",
    "            \n",
    "            sentiment_data[symbol] = pd.DataFrame(sentiment_scores, index=dates)\n",
    "            \n",
    "        self.sentiment_data = sentiment_data\n",
    "        return sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    \"\"\"Advanced data cleaning and preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.anomaly_detectors = {}\n",
    "        self.cleaning_stats = {}\n",
    "    \n",
    "    def remove_outliers(self, df, columns, method='isolation_forest'):\n",
    "        \"\"\"Remove outliers using multiple methods\"\"\"\n",
    "        print(\"üßπ Removing outliers...\")\n",
    "        \n",
    "        if method == 'isolation_forest':\n",
    "            iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "            outliers = iso_forest.fit_predict(df[columns])\n",
    "            return df[outliers == 1]\n",
    "        \n",
    "        elif method == 'zscore':\n",
    "            z_scores = stats.zscore(df[columns])\n",
    "            return df[(z_scores < 3).all(axis=1)]\n",
    "        \n",
    "        elif method == 'iqr':\n",
    "            Q1 = df[columns].quantile(0.25)\n",
    "            Q3 = df[columns].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            return df[~((df[columns] < (Q1 - 1.5 * IQR)) | (df[columns] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    \n",
    "    def handle_missing_values(self, df, method='interpolate'):\n",
    "        \"\"\"Handle missing values with multiple methods\"\"\"\n",
    "        print(\"üîç Handling missing values...\")\n",
    "        \n",
    "        if method == 'interpolate':\n",
    "            return df.interpolate(method='cubic')\n",
    "        \n",
    "        elif method == 'forward_fill':\n",
    "            return df.fillna(method='ffill')\n",
    "        \n",
    "        elif method == 'backward_fill':\n",
    "            return df.fillna(method='bfill')\n",
    "            \n",
    "    def normalize_features(self, df, columns, method='robust'):\n",
    "        \"\"\"Normalize features using multiple methods\"\"\"\n",
    "        print(\"üìä Normalizing features...\")\n",
    "        \n",
    "        if method == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        elif method == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif method == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "            \n",
    "        df[columns] = scaler.fit_transform(df[columns])\n",
    "        return df\n",
    "    \n",
    "    def check_stationarity(self, series):\n",
    "        \"\"\"Check time series stationarity\"\"\"\n",
    "        print(\"üìà Checking stationarity...\")\n",
    "        \n",
    "        # Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(series)\n",
    "        \n",
    "        # KPSS test\n",
    "        kpss_result = kpss(series)\n",
    "        \n",
    "        return {\n",
    "            'adf_statistic': adf_result[0],\n",
    "            'adf_pvalue': adf_result[1],\n",
    "            'kpss_statistic': kpss_result[0],\n",
    "            'kpss_pvalue': kpss_result[1]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class AdvancedFeatureEngineering:\n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def generate_features(self, df, symbol):\n",
    "        \"\"\"Generate comprehensive feature set\"\"\"\n",
    "        print(f\"‚öôÔ∏è Engineering features for {symbol}...\")\n",
    "        \n",
    "        # Technical Indicators\n",
    "        df['MA_7'] = talib.MA(df['Close'], timeperiod=7)\n",
    "        df['MA_21'] = talib.MA(df['Close'], timeperiod=21)\n",
    "        df['RSI'] = talib.RSI(df['Close'], timeperiod=14)\n",
    "        df['MACD'], df['MACD_Signal'], _ = talib.MACD(df['Close'])\n",
    "        df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = talib.BBANDS(df['Close'])\n",
    "        df['ADX'] = talib.ADX(df['High'], df['Low'], df['Close'])\n",
    "        df['OBV'] = talib.OBV(df['Close'], df['Volume'])\n",
    "        \n",
    "        # Momentum Indicators\n",
    "        df['ROC'] = talib.ROC(df['Close'], timeperiod=10)\n",
    "        df['MOM'] = talib.MOM(df['Close'], timeperiod=10)\n",
    "        df['CCI'] = talib.CCI(df['High'], df['Low'], df['Close'], timeperiod=14)\n",
    "        \n",
    "        # Volatility Indicators\n",
    "        df['ATR'] = talib.ATR(df['High'], df['Low'], df['Close'], timeperiod=14)\n",
    "        df['NATR'] = talib.NATR(df['High'], df['Low'], df['Close'], timeperiod=14)\n",
    "        \n",
    "        # Volume Indicators\n",
    "        df['AD'] = talib.AD(df['High'], df['Low'], df['Close'], df['Volume'])\n",
    "        df['ADOSC'] = talib.ADOSC(df['High'], df['Low'], df['Close'], df['Volume'])\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Advanced model training with ensemble methods and validation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.performance_metrics = {}\n",
    "        self.validation_results = {}\n",
    "        \n",
    "    def create_sequences(self, data, seq_length):\n",
    "        \"\"\"Create sequences for time series prediction\"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            X.append(data[i:(i + seq_length)])\n",
    "            y.append(data[i + seq_length])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def train_lstm_model(self, X_train, y_train):\n",
    "        \"\"\"Train LSTM model with attention\"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(128, return_sequences=True),\n",
    "            Attention(),\n",
    "            LSTM(64),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def train_ensemble(self, X_train, y_train):\n",
    "        \"\"\"Train ensemble of models\"\"\"\n",
    "        models = {\n",
    "            'lightgbm': lgb.LGBMRegressor(),\n",
    "            'xgboost': xgb.XGBRegressor(),\n",
    "            'catboost': cb.CatBoostRegressor(verbose=False),\n",
    "            'random_forest': RandomForestRegressor()\n",
    "        }\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            self.models[name] = model\n",
    "            \n",
    "    def validate_models(self, X_test, y_test):\n",
    "        \"\"\"Validate models with multiple metrics\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            predictions = model.predict(X_test)\n",
    "            \n",
    "            metrics[name] = {\n",
    "                'mse': mean_squared_error(y_test, predictions),\n",
    "                'r2': r2_score(y_test, predictions),\n",
    "                'mape': mean_absolute_percentage_error(y_test, predictions)\n",
    "            }\n",
    "            \n",
    "        self.validation_results = metrics\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MarketPredictor:\n",
    "    def __init__(self):\n",
    "        self.data_collector = DataCollector()\n",
    "        self.feature_engineer = AdvancedFeatureEngineering()\n",
    "        self.models = {}\n",
    "        \n",
    "    def prepare_training_data(self, symbols, start_date, end_date):\n",
    "        \"\"\"Prepare comprehensive training dataset\"\"\"\n",
    "        \n",
    "        # Fetch all required data\n",
    "        historical_data = self.data_collector.fetch_historical_data(symbols, start_date, end_date)\n",
    "        economic_data = self.data_collector.add_economic_indicators()\n",
    "        sentiment_data = self.data_collector.add_sentiment_analysis(symbols)\n",
    "        \n",
    "        # Process each symbol\n",
    "        processed_data = {}\n",
    "        for symbol in symbols:\n",
    "            df = historical_data[symbol].copy()\n",
    "            \n",
    "            # Add features\n",
    "            df = self.feature_engineer.generate_features(df, symbol)\n",
    "            \n",
    "            # Merge with economic and sentiment data\n",
    "            df = df.join(economic_data, how='left')\n",
    "            df = df.join(sentiment_data[symbol], how='left')\n",
    "            \n",
    "            # Handle missing values\n",
    "            df = df.fillna(method='ffill')\n",
    "            \n",
    "            processed_data[symbol] = df\n",
    "            \n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define parameters\n",
    "    symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA']\n",
    "    start_date = '2010-01-01'\n",
    "    end_date = '2024-01-01'\n",
    "    \n",
    "    # Initialize predictor\n",
    "    predictor = MarketPredictor()\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\nüöÄ Initializing Advanced Market Prediction Pipeline\")\n",
    "    print(\"================================================\")\n",
    "    \n",
    "    processed_data = predictor.prepare_training_data(symbols, start_date, end_date)\n",
    "    \n",
    "    # Display sample of processed data\n",
    "    for symbol in symbols:\n",
    "        print(f\"\\nüìà Sample of processed data for {symbol}:\")\n",
    "        print(processed_data[symbol].tail().round(2))\n",
    "        \n",
    "        # Plot key metrics\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=processed_data[symbol].index,\n",
    "            y=processed_data[symbol]['Close'],\n",
    "            name=\"Price\",\n",
    "            line=dict(color='blue')\n",
    "        ))\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=processed_data[symbol].index,\n",
    "            y=processed_data[symbol]['MA_21'],\n",
    "            name=\"21-day MA\",\n",
    "            line=dict(color='red', dash='dash')\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"{symbol} Price and Moving Average\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Price\",\n",
    "            template=\"plotly_dark\"\n",
    "        )\n",
    "        \n",
    "        fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
